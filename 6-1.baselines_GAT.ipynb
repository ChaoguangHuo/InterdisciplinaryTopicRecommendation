{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef45d5a-4f5c-407b-b102-fa4b1c17cd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import data_generator\n",
    "import argparse\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random\n",
    "torch.set_num_threads(2)\n",
    "import os\n",
    "import re\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d9bfa3-64ba-4196-9927-3277c9e0dac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description = 'data generation')\n",
    "parser.add_argument('--A_n', type = int, default = 7876,\n",
    "               help = 'number of author node')\n",
    "parser.add_argument('--P_n', type = int, default = 14518,\n",
    "               help = 'number of paper node')\n",
    "parser.add_argument('--V_n', type = int, default =38542 ,\n",
    "               help = 'number of venue node')\n",
    "#parser.add_argument('--C_n', type = int, default = 4,\n",
    "#\t\t\t   help = 'number of node class label')\n",
    "parser.add_argument('--data_path', type = str, default = r'D:\\11.12_Topic_Recommendation\\data',\n",
    "\t\t\t\t   help='path to data')\n",
    "parser.add_argument('--model_path', type = str, default = r'D:\\11.12_Topic_Recommendation\\model',\n",
    "\t\t\t\t   help='path to save model')\n",
    "parser.add_argument('--walk_n', type = int, default = 10,\n",
    "\t\t\t   help='number of walk per root node')\n",
    "parser.add_argument('--walk_L', type = int, default = 10,\n",
    "\t\t\t   help='length of each walk')\n",
    "parser.add_argument('--window', type = int, default = 7,\n",
    "\t\t\t   help='window size for relation extration')\n",
    "parser.add_argument('--T_split', type = int, default = 2023,\n",
    "\t\t\t   help = 'split time of train/test data')\n",
    "parser.add_argument('--in_f_d', type = int, default = 128,\n",
    "                       help = 'input feature dimension')\n",
    "parser.add_argument('--embed_d', type = int, default = 128,\n",
    "                       help = 'embedding dimension')\n",
    "parser.add_argument('--lr', type = int, default = 0.001,\n",
    "                       help = 'learning rate')\n",
    "parser.add_argument('--batch_s', type = int, default = 20000,\n",
    "               help = 'batch size')\n",
    "parser.add_argument('--mini_batch_s', type = int, default = 200,\n",
    "               help = 'mini batch size')\n",
    "parser.add_argument('--train_iter_n', type = int, default = 50,\n",
    "               help = 'max number of training iteration')\n",
    "parser.add_argument(\"--random_seed\", default = 10, type = int)\n",
    "parser.add_argument('--train_test_label', type= int, default = 0,\n",
    "               help='train/test label: 0 - train, 1 - test, 2 - code test/generate negative ids for evaluation')\n",
    "parser.add_argument('--save_model_freq', type = float, default = 2,\n",
    "               help = 'number of iterations to save model')\n",
    "parser.add_argument(\"--cuda\", default = 0, type = int)\n",
    "parser.add_argument(\"--checkpoint\", default = '', type=str)\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)\n",
    "random.seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "torch.cuda.manual_seed_all(args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232afd48-f95b-4b40-8961-cdf17eeeb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Edges\n",
    "\n",
    "a_p_list_train = [[] for k in range(args.A_n)]\n",
    "a_p_list_test = [[] for k in range(args.A_n)]\n",
    "p_a_list_train = [[] for k in range(args.P_n)]\n",
    "p_a_list_test = [[] for k in range(args.P_n)]\n",
    "#p_p_cite_list_train = [[] for k in range(self.args.P_n)]\n",
    "#p_p_cite_list_test = [[] for k in range(self.args.P_n)]\n",
    "v_p_list_train = [[] for k in range(args.V_n)]\n",
    "\n",
    "#relation_f = [\"a_p_list_train.txt\", \"a_p_list_test.txt\", \"p_a_list_train.txt\", \"p_a_list_test.txt\",\\\n",
    "# \"p_p_cite_list_train.txt\", \"p_p_cite_list_test.txt\", \"v_p_list_train.txt\"]\n",
    "relation_f = [r\"\\a_p_list_train.txt\", r\"\\a_p_list_test.txt\", r\"\\p_a_list_train.txt\", r\"\\p_a_list_test.txt\",\\\n",
    "  r\"\\v_p_list_train.txt\"]\n",
    "\n",
    "#store academic relational data \n",
    "for i in range(len(relation_f)):\n",
    "    f_name = relation_f[i]\n",
    "    neigh_f = open(args.data_path + f_name, \"r\")\n",
    "    for line in neigh_f:\n",
    "        line = line.strip()\n",
    "        node_id = int(re.split(':', line)[0])\n",
    "        neigh_list = re.split(':', line)[1]\n",
    "        neigh_list_id = re.split(',', neigh_list)\n",
    "        if f_name == r'\\a_p_list_train.txt':\n",
    "            for j in range(len(neigh_list_id)):\n",
    "                a_p_list_train[node_id].append('p'+str(neigh_list_id[j]))\n",
    "        elif f_name == r'\\a_p_list_test.txt':\n",
    "            for j in range(len(neigh_list_id)):\n",
    "                a_p_list_test[node_id].append('p'+str(neigh_list_id[j]))\n",
    "        elif f_name == r'\\p_a_list_train.txt':\n",
    "            for j in range(len(neigh_list_id)):\n",
    "                p_a_list_train[node_id].append('a'+str(neigh_list_id[j]))\n",
    "        elif f_name == r'\\p_a_list_test.txt':\n",
    "            for j in range(len(neigh_list_id)):\n",
    "                p_a_list_test[node_id].append('a'+str(neigh_list_id[j]))\n",
    "        #elif f_name == 'p_p_cite_list_train.txt':\n",
    "        #\tfor j in range(len(neigh_list_id)):\n",
    "        #\t\tp_p_cite_list_train[node_id].append('p'+str(neigh_list_id[j]))\n",
    "        #elif f_name == 'p_p_cite_list_test.txt':\n",
    "        #\tfor j in range(len(neigh_list_id)):\n",
    "        #\t\tp_p_cite_list_test[node_id].append('p'+str(neigh_list_id[j]))\n",
    "        else:\n",
    "            for j in range(len(neigh_list_id)):\n",
    "                v_p_list_train[node_id].append('p'+str(neigh_list_id[j]))\n",
    "    neigh_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b9273-784b-48dd-808f-264b93c12624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Edges\n",
    "\n",
    "p_v = [[] for k in range(args.P_n)]\n",
    "p_v_f = open(args.data_path + r'\\p_v.txt', \"r\")\n",
    "for line in p_v_f:\n",
    "    line = line.strip()\n",
    "    node_id = int(re.split(':', line)[0])\n",
    "    neigh_list = re.split(':', line)[1]\n",
    "    neigh_list_id = re.split(',', neigh_list)\n",
    "    for j in range(len(neigh_list_id)):\n",
    "        p_v[node_id].append('v'+str(neigh_list_id[j]))\n",
    "p_v_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315daa2a-6fa5-4fa0-84f0-64394b6ad6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import *\n",
    "import argparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc37e2-a1cb-44cd-b7b3-3fc2a6578a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store paper content pre-trained embedding(abstract_title)\n",
    "p_abstract_embed = np.zeros((args.P_n, args.in_f_d))\n",
    "p_a_e_f = open(args.data_path + r\"\\p_abstract_embed.txt\", \"r\")\n",
    "for line in islice(p_a_e_f, 1, None):\n",
    "    values = line.split()\n",
    "    index = int(values[0])\n",
    "    embeds = np.asarray(values[1:], dtype='float32')\n",
    "    p_abstract_embed[index] = embeds\n",
    "p_a_e_f.close()\n",
    "\n",
    "p_title_embed = np.zeros((args.P_n, args.in_f_d))\n",
    "p_t_e_f = open(args.data_path + r\"\\p_title_embed.txt\", \"r\")\n",
    "for line in islice(p_t_e_f, 1, None):\n",
    "    values = line.split()\n",
    "    index = int(values[0])\n",
    "    embeds = np.asarray(values[1:], dtype='float32')\n",
    "    p_title_embed[index] = embeds\n",
    "p_t_e_f.close()\n",
    "\n",
    "p_abstract_embed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3053dcb-8721-46e1-a80a-bc4f3dd9a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store structure pre-trained embedding\n",
    "a_net_embed = np.zeros((args.A_n,args.in_f_d))\n",
    "p_net_embed = np.zeros((args.P_n, args.in_f_d))\n",
    "v_net_embed = np.zeros((args.V_n, args.in_f_d)) \n",
    "net_e_f = open(args.data_path + r\"\\node_net_embedding_10.txt\", \"r\")\n",
    "for line in islice(net_e_f, 1, None):\n",
    "    line = line.strip()\n",
    "    index = re.split(' ', line)[0]\n",
    "    if len(index) and (index[0] == 'a' or index[0] == 'v' or index[0] == 'p'):\n",
    "        embeds = np.asarray(re.split(' ', line)[1:], dtype='float32')\n",
    "        #embeds = np.asarray([0]*128, dtype='float32')\n",
    "        if index[0] == 'a':\n",
    "            a_net_embed[int(index[1:])] = embeds\n",
    "        elif index[0] == 'v':\n",
    "            v_net_embed[int(index[1:])] = embeds\n",
    "        else:\n",
    "            p_net_embed[int(index[1:])] = embeds\n",
    "net_e_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed6ba9-994d-4e10-a2bc-34abeb56ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get structure embedding of P\n",
    "\n",
    "p_a_net_embed = np.zeros((args.P_n, args.in_f_d))\n",
    "for i in range(args.P_n):\n",
    "    if len(p_a_list_train[i]):\n",
    "        for j in range(len(p_a_list_train[i])):\n",
    "            a_id = int(p_a_list_train[i][j][1:])\n",
    "            p_a_net_embed[i] = np.add(p_a_net_embed[i], a_net_embed[a_id])\n",
    "        p_a_net_embed[i] = p_a_net_embed[i] / len(p_a_list_train[i])\n",
    "\n",
    "p_v_net_embed = np.zeros((args.P_n, args.in_f_d))\n",
    "for i in range(args.P_n):\n",
    "    if len(p_v[i]):\n",
    "        for j in range(len(p_v[i])):\n",
    "            v_id = int(p_v[i][j][1:])\n",
    "            p_v_net_embed[i] = np.add(p_v_net_embed[i], v_net_embed[v_id])\n",
    "        p_v_net_embed[i] = p_v_net_embed[i] / len(p_v[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8565a-1e43-4735-8886-0d6607b0e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#empirically use 5 paper embedding for author content embeding generation\n",
    "a_text_embed = np.zeros((args.A_n, args.in_f_d * 5))\n",
    "for i in range(args.A_n):\n",
    "    if len(a_p_list_train[i]):\n",
    "        feature_temp = []\n",
    "        if len(a_p_list_train[i]) >= 5:\n",
    "            #id_list_temp = random.sample(a_p_list_train[i], 5)\n",
    "            for j in range(5):\n",
    "                feature_temp.append(p_abstract_embed[int(a_p_list_train[i][j][1:])])\n",
    "        else:\n",
    "            for j in range(len(a_p_list_train[i])):\n",
    "                feature_temp.append(p_abstract_embed[int(a_p_list_train[i][j][1:])])\n",
    "            for k in range(len(a_p_list_train[i]), 5):\n",
    "                feature_temp.append(p_abstract_embed[int(a_p_list_train[i][-1][1:])])\n",
    "\n",
    "        feature_temp = np.reshape(np.asarray(feature_temp), [1, -1])\n",
    "        a_text_embed[i] = feature_temp\n",
    "\n",
    "#empirically use 5 paper embedding for author content embeding generation\n",
    "v_text_embed = np.zeros((args.V_n, args.in_f_d * 5))\n",
    "for i in range(args.V_n):\n",
    "    if len(v_p_list_train[i]):\n",
    "        feature_temp = []\n",
    "        if len(v_p_list_train[i]) >= 5:\n",
    "            #id_list_temp = random.sample(a_p_list_train[i], 5)\n",
    "            for j in range(5):\n",
    "                feature_temp.append(p_abstract_embed[int(v_p_list_train[i][j][1:])])\n",
    "        else:\n",
    "            for j in range(len(v_p_list_train[i])):\n",
    "                feature_temp.append(p_abstract_embed[int(v_p_list_train[i][j][1:])])\n",
    "            for k in range(len(v_p_list_train[i]), 5):\n",
    "                feature_temp.append(p_abstract_embed[int(v_p_list_train[i][-1][1:])])\n",
    "\n",
    "        feature_temp = np.reshape(np.asarray(feature_temp), [1, -1])\n",
    "        v_text_embed[i] = feature_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc955ef5-8785-4130-85d1-d9d0601de3a8",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12928ff0-6706-4012-9399-3948551ed31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
